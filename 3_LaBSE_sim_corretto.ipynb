{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm #library that creates a progression bar in the terminal\n",
    "\n",
    "# Download BERT LaBSE model\n",
    "tokenizer_labse = BertTokenizer.from_pretrained(\"sentence-transformers/LaBSE\")\n",
    "model = BertModel.from_pretrained(\"sentence-transformers/LaBSE\", device_map='cuda') \n",
    "#cuda allows me to use GPU with Pytorch\n",
    "nltk.download('punkt') # nltk's sentencizer (module) that tokenizes sentences when it encounters a punctuation sign\n",
    "\n",
    "# Function to split text into sentences\n",
    "def split_into_sentences(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "# Function to get the LABSE embeddings for each sentence\n",
    "def get_labse_embedding(sentences, batch_size=8): \n",
    "    # batch_size indicates the size of each batch of sentences processed at once. Allows us to control memory usage\n",
    "    embeddings = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i + batch_size] #selects a batch of sentences from the sentences list based on the current index i and the batch size.\n",
    "        tokens = tokenizer_labse(batch, return_tensors='pt', padding=True, truncation=True)\n",
    "        tokens = {k: tokens[k].to('cuda') for k in tokens.keys()} #We have a set of tensors, each with a label (key). To perform computations swiftly, we have to use a powerful machine (GPU). The code takes each tensor, sends it to the powerful machine, and updates the set to reflect this move. The code iterates over all items, sends each one to the fast machine, and updates the collection accordingly.\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens)\n",
    "        batch_embeddings = outputs.last_hidden_state.mean(dim=1) #calculates the mean pooling of the last hidden states across all tokens in each sentence. Results in a single vector representation (embedding) for each sentence in the batch.\n",
    "        embeddings.append(batch_embeddings) #appends the embeddings of the sentences in the current batch to the embeddings list. Each element of embeddings is a tensor containing the embeddings of one batch of sentences.\n",
    "    return torch.cat(embeddings) #all the embeddings in the embeddings list along the specified dimension (default is 0), resulting in a single tensor containing embeddings for all sentences in the input list.\n",
    "# This ensures that we get a single embedding of a fixed size for each sentence.\n",
    "\n",
    "# Function to calculate cosine similarity\n",
    "def calculate_cosine_similarity(embeddings1, embeddings2):\n",
    "    similarity = cosine_similarity(embeddings1.cpu(), embeddings2.cpu())\n",
    "    return similarity\n",
    "\n",
    "# Function to process the subfolder corresponding to a single entry in multiple languages\n",
    "# It also initializes data structures (a dictionary) to store sentences by language, along with a variable to track the maximum number of sentences.\n",
    "def process_subdirectory(directory, batch_size=8):\n",
    "    files = os.listdir(directory)\n",
    "    sentences_by_language = {}\n",
    "    max_sentences = 0\n",
    "\n",
    "    # Group texts by language and store the maximum number of sentences\n",
    "    for file in files:\n",
    "        match = re.search(r'_([a-zA-Z]{2})\\.txt$', file)\n",
    "        if match:\n",
    "            language = match.group(1)\n",
    "            if language not in sentences_by_language:\n",
    "                sentences_by_language[language] = []\n",
    "            with open(os.path.join(directory, file), 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                sentences = split_into_sentences(text)\n",
    "                sentences_by_language[language].extend(sentences)\n",
    "                #Update the maimum number of sentences\n",
    "                max_sentences = max(max_sentences, len(sentences))\n",
    "\n",
    "    # Calculate embeddings for each language #DIF\n",
    "    embeddings_by_language = {}\n",
    "    for language, sentences in sentences_by_language.items():\n",
    "        sentence_embeddings = get_labse_embedding(sentences, batch_size=batch_size) # troviamo i tensori degli embedding delle frasi\n",
    "        pad_length = max_sentences - len(sentences) # troviamo la lunghezza di padding\n",
    "        zero_tensor = torch.zeros((pad_length, sentence_embeddings.shape[1])).to('cuda') if pad_length > 0 else torch.tensor([], device='cuda')\n",
    "        embeddings_by_language[language] = torch.cat([sentence_embeddings, zero_tensor])\n",
    "\n",
    "    # Calculate cosine similarity between all pairs of sentences #Da spiegare\n",
    "    similarities = []\n",
    "    languages = list(embeddings_by_language.keys())\n",
    "    for i in range(len(languages)):\n",
    "        lang1 = languages[i]\n",
    "        for j in range(i + 1, len(languages)):\n",
    "            lang2 = languages[j]\n",
    "            similarity = calculate_cosine_similarity(embeddings_by_language[lang1], embeddings_by_language[lang2])\n",
    "            for k in range(similarity.shape[0]):\n",
    "                for l in range(similarity.shape[1]):\n",
    "                    if k < len(sentences_by_language[lang1]) and l < len(sentences_by_language[lang2]):\n",
    "                        similarities.append({\n",
    "                            'Language 1': lang1,\n",
    "                            'Sentence 1': sentences_by_language[lang1][k],\n",
    "                            'Language 2': lang2,\n",
    "                            'Sentence 2': sentences_by_language[lang2][l],\n",
    "                            'Cosine similarity': similarity[k][l]\n",
    "                        })\n",
    "\n",
    "    # Create a DataFrame with the results and return it\n",
    "    df = pd.DataFrame(similarities)\n",
    "    return df\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    main_directory = \"Scienza_puliti\"\n",
    "    subdirectories = os.listdir(main_directory)\n",
    "    sim_dir = \"Prova_full\"\n",
    "\n",
    "    if not os.path.exists(sim_dir):\n",
    "        os.makedirs(sim_dir)\n",
    "\n",
    "    progbar = tqdm(subdirectories, total=len(subdirectories))\n",
    "    for subdirectory in progbar:\n",
    "        progbar.set_description(desc=f'Current subdirectory: {subdirectory}')\n",
    "        subdirectory_path = os.path.join(main_directory, subdirectory)\n",
    "        if os.path.isdir(subdirectory_path):\n",
    "            df = process_subdirectory(subdirectory_path)\n",
    "            csv_filename = f\"{subdirectory}.csv\"\n",
    "            df.to_csv(os.path.join(sim_dir, csv_filename), index=False)\n",
    "            torch.cuda.empty_cache()  # Clear GPU memory\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
